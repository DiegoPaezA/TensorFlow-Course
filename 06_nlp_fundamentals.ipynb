{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://tinyurl.com/2khrsfbc\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with TensorFlow\n",
    "\n",
    "El objetivo principal del [procesamiento de lenguaje natural](https://tinyurl.com/y68tj6v6) (NLP) es analizar, comprender y derivar información valiosa de texto. Las aplicaciones de NLP pueden clasificarse en dos categorías principales:\n",
    "\n",
    "- Texto (Como el que se encuentra en un email, un tweet, un documento, etc.)\n",
    "- Audio (Como el que se encuentra en una grabación de voz, una canción, etc.)\n",
    "\n",
    "Por ejemplo, \n",
    "\n",
    "- Si tuviéramos un conjunto de datos de texto que contuviera reseñas de películas, podríamos usar NLP para identificar el sentimiento detrás de cada reseña (por ejemplo, positivo o negativo). \n",
    "- También podríamos usar NLP para identificar el tema de cada reseña (por ejemplo, romance, acción, etc.). Si quisieramos construir un identificador de spam, podríamos usar NLP para identificar palabras clave en un correo electrónico que podrían indicar que es spam.\n",
    "- Generación de texto: Podríamos usar NLP para generar texto, como un resumen de un artículo, un título de un artículo, una respuesta a un correo electrónico, etc.\n",
    "\n",
    "Cuando se trabaja con texto o audio en NLP los datos tienen un orden secuencial por lo tanto el tratamiento que reciben los datos es diferente al de los datos de imágenes o tabulares.\n",
    "\n",
    "**Aplicaciones del Modelado de Datos Secuenciales**\n",
    "\n",
    "Al hablar de redes neuronales densas se tenía una información de entrada y se esperaba una salida sin importar el tiempo ni el orden en el que llega la información a la red. En el modelado de datos secuenciales, el orden de la información es importante:\n",
    "\n",
    "1. En el primer ejemplo se tienen varias entradas y se obtiene una salida. \n",
    "2. En el segundo ejemplo se tiene una entrada y se quiere obtener una secuencia que represente información asociada a la acción entrante. \n",
    "3. En el ultimo ejemplo se tiene una secuencia de entrada y se espera obtener una secuencia de salida.\n",
    "\n",
    "<a href=\"https://ibb.co/m0LKSmf\"><img src=\"https://i.ibb.co/jzcYHdn/nlp-problems.png\" alt=\"nlp-problems\" border=\"0\"></a>\n",
    "\n",
    "Uno de los factores a tener en cuenta al trabajar con datos secuenciales, es que las redes neuronales densas no son capaces de capturar la información de secuencia. Por lo tanto, se requiere una red neuronal que sea capaz de capturar la información de secuencia. Para ello se utilizan las **redes neuronales recurrentes**.\n",
    "\n",
    "Las **redes neuronales recurrentes (RNN**): son redes neuronales que tienen una memoria interna que les permite capturar la información de secuencia. La arquitecura base de las RNN es la siguiente:\n",
    "\n",
    "<a href=\"https://ibb.co/dg8ycMy\"><img src=\"https://i.ibb.co/9tQjvgj/arq-rnn.png\" alt=\"arq-rnn\" border=\"0\"></a>\n",
    "\n",
    "<a href=\"https://ibb.co/TrkZFZS\"><img src=\"https://i.ibb.co/yVXDbD1/imagen-2023-02-13-105448726.png\" alt=\"imagen-2023-02-13-105448726\" border=\"0\"></a>\n",
    "\n",
    ">**En este notebook, se trabará sobre lo siguiente**:\n",
    "\n",
    "- Descargar y explorar un conjunto de datos de texto\n",
    "- Visualizar datos de texto\n",
    "- Convertir texto en números usando tokenización\n",
    "- Convertir texto tokenizado en vectores usando codificación embebida\n",
    "- Modelar un dataset de texto\n",
    "- Predecir el sentimiento de una oración\n",
    "    - Iniciar con una referencia (TF-IDF)\n",
    "    - Construir diferentes modelos de texto de deep learning\n",
    "        - Dense, LSTM, GRU, Conv1D, Transfer Learning\n",
    "- Comparar el desempeño de cada modelo\n",
    "- Combinar los en un ensamble\n",
    "- Guardar y cargar un modelo entrenado\n",
    "- Encontrar las peores predicciones.\n",
    "\n",
    "<a href=\"https://ibb.co/yYkdqg3\"><img src=\"https://i.ibb.co/7RpjKr9/nlp-experiments.png\" alt=\"nlp-experiments\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2060 with Max-Q Design (UUID: GPU-9970422a-f4b7-7ab0-4f13-419e26e877d3)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descargar y explorar un dataset de texto\n",
    "\n",
    "Se utilizará el dataset [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) que contiene Tweets acerca de desastres naturales. El objetivo es predecir si un tweet es acerca de un desastre real o no.\n",
    "\n",
    "- Un Tweet **Real** es acerca de un desastre real, por ejemplo:\n",
    "> Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n",
    "\n",
    "- Un Tweet **No Real** no es acerca de un desastre real pueden ser sobre cualquier cosa, por ejemplo:\n",
    "> 'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n",
    "\n",
    "Al descomprimir el dataset se obtendran tres archivos:\n",
    "\n",
    "- `sample_submission.csv` - un archivo de muestra de la estructura de envío de predicciones en las competencias de Kaggle.\n",
    "- `train.csv` - el conjunto de datos de entrenamiento que contiene los tweets reales y no reales de desastres etiquetados.\n",
    "- `test.csv` - el conjunto de datos de prueba que contiene los tweets reales y no reales que se deben predecir.\n",
    "\n",
    "\n",
    "### 1.1 Visualizar un dataset de texto\n",
    "\n",
    "Una vez que se cuenta con un dataset para trabajar, es una buena idea visualizarlo para ver si hay algo que se pueda entender de él. El dataset se encuentra en archivos `.csv`, una forma sencilla de hacerlos legibles es usando la función `pd.read_csv()` de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"datasets/nlp_real_not_dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"datasets/nlp_real_not_dataset/test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "los datos descargados seguramente se encuentran mezclados. Pero para asegurarnos es una buena idea volverlos a mezclar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de entrenamiento contienen la columna `target` que indica si un tweet es real o no real. Esta columna será la etiqueta que se usará para entrenar el modelo. El dataset de prueba no contiene esta columna, ya que es lo que se debe predecir.\n",
    "\n",
    "La estructura que se quiere implementar es la siguiente:\n",
    "\n",
    "> Inputs (text column) -> Machine Learning Algorithm -> Outputs (target column)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-text-classification-inputs-and-outputs.png\" alt=\"nlp\" border=\"0\" width = 800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test data doesn't have a target (that's what we'd try to predict)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que se cuenta con dos valores objetivo (0 y 1), se trata de un problema de clasificación binaria. El dataset se encuentra desbalanceado, con un 60% de tweets de clase 0 y un 40% de tweets de clase 1.\n",
    "\n",
    "donde: \n",
    "\n",
    "- 1 = Un tweet es acerca de un desastre real\n",
    "- 0 = Un tweet no es acerca de un desastre real\n",
    "\n",
    "El dataset de entrenamiento contiene 7613 tweets, el dataset de prueba contiene 3263 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "# How many samples total?\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "I'm not the mom friend but I still see my friends as my little babies that I have to care for or else they'll electrocute themselves\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "@OKgooner hahaha great song. 'Spent 15 years getting loaded. 15 years till his liver exploded. Now what's Bob going to do NOW that he...'\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Three Israeli soldiers wounded in West Bank terrorist attack via /r/worldnews http://t.co/su4ZVWADj7\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/wvTPuRYx63 #prebreak #best\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "How is it one careless match can start a forest fire but it takes a whole box to start a campfire?\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualización de algunos de los ejemplos de entrenamiento\n",
    "import random\n",
    "random_index = random.randint(0,len(train_df)-5)\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _,text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\"\n",
    "          if target>0 else \"(not real disaster)\"  \n",
    "    )\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Separar el dataset en conjuntos de entrenamiento y validación\n",
    "\n",
    "Debido a que el dataset de prueba no contiene la columna `target`, se debe separar el dataset de entrenamiento en dos conjuntos: uno para entrenar el modelo y otro para validar el modelo. Para ello se utilizará la función [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42) # random state for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificar los primeros 10 ejemplos\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convertir texto en números\n",
    "\n",
    "Para poder alimentar texto a un modelo de deep learning, es necesario convertirlo en números. Hay varias formas de hacer esto, las más comunes son:\n",
    "\n",
    "- **Tokenización**: Mapeado directo de palabras, caracteres o símbolos a números enteros. Hay tres niveles de tokenización:\n",
    "    - **Palabra**: convertir cada palabra en una secuencia en un número entero. Con la sentencia \"i love tensorflow\", la palabra \"i\" se convertiría en el número 1, \"love\" en el número 2 y \"tensorflow\" en el número 3.\n",
    "    - **Caracter**: convertir cada caracter en una secuencia en un número entero. Como convertir el abecedarios A-Z en números del 1 al 26.\n",
    "    - **Subpalabra**: convertir cada subpalabra en una secuencia en un número entero. Involucra dividir las palabras en subpalabras, por ejemplo, \"tensorflow\" se dividiría en \"ten\", \"sor\", \"flo\", \"w\".\n",
    "\n",
    "\n",
    "- **Codificación embebida**: Es una representación de lenguaje natural que puede ser aprendida. La representación se da en forma de un vector de características de longitud fija, donde cada característica representa un aspecto del lenguaje. Por ejemplo, la palabra \"dance\" puede ser presentada por un vector de 5 dimensiones [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. Es importante resaltar que el tamaño del vector es ajustable. Existen dos formas de usar la codificación embebida:\n",
    "    - **Codificación embebida preentrenada**: Se utiliza una codificación embebida que ya ha sido entrenada en un conjunto de datos grande. Por ejemplo, la codificación embebida preentrenada de [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
    "    - **Codificación embebida entrenada**: Se entrena una codificación embebida desde cero en un conjunto de datos específico. Por ejemplo, se entrena una codificación embebida en un conjunto de datos de tweets.\n",
    "\n",
    "<a href=\"https://ibb.co/0sJqwkD\"><img src=\"https://i.ibb.co/Z2TdZ3L/tokens.png\" alt=\"tokens\" border=\"0\"></a>\n",
    "\n",
    ">:key: **Nota**: Qué nivel de tokenización o codificación embebida se debe utilizar? \n",
    "\n",
    "Depende del problema que se está tratando de resolver. Es importante experimentar con diferentes niveles de tokenización y codificación embebida para ver cuál funciona mejor para tu problema.\n",
    "\n",
    "Algunas herramientas de codificación embebida preentrenadas:\n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [Word2Vec](https://jalammar.github.io/illustrated-word2vec/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">:toolbox: **Vectorización de texto (tokenización)**\n",
    "\n",
    "Para vectorizar el texto, se utilizará la clase [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) de TensorFlow. Esta clase convierte una secuencia de caracteres en una secuencia de enteros, donde cada entero representa un token en un diccionario.\n",
    "\n",
    "La capa `TextVectorization` toma los siguientes parametros: \n",
    "\n",
    "- `max_tokens` - el número máximo de tokens a tener en el diccionario. Si se establece en `None`, el diccionario tendrá tantos tokens como palabras únicas en el texto de entrenamiento.\n",
    "- `standardize` - la función de normalización a aplicar a los datos de entrada. Por defecto, la función de normalización convierte el texto a minúsculas y elimina la puntuación.\n",
    "- `split` - la función de separación a aplicar a los datos de entrada. Por defecto, \"whitespace\" separa el texto en palabras.\n",
    "- `ngrams` - Cuantas palabras se deben combinar en un solo token. Por ejemplo, si `ngrams` es 2, la frase \"I love TensorFlow\" se convertiría en \"I love\", \"love TensorFlow\".\n",
    "- `output_mode` - el modo de salida de la capa. Por defecto, \"int\" devuelve una secuencia de enteros (cada entero representa un token en el diccionario). `int` (mapeado entero), `binary` (one-hot encoding), `count` o `tf-idf`.\n",
    "- `output_sequence_length` - Tamaño de la secuencia de salida tokenizada. Por ejemplo, si `output_sequence_length` es 10, la secuencia de salida tendrá 10 tokens, si la secuencia de entrada tiene menos de 10 tokens, se rellenará con ceros. Si la secuencia de entrada tiene más de 10 tokens, se truncará a los primeros 10 tokens.\n",
    "- `pad_to_max_tokens` - Si es `True`, la secuencia de salida se rellenará con ceros hasta que tenga el tamaño de `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La inicialización anterior de la capa `TextVectorization` utiliza los parametros por defecto. A continuación se creará una nueva capa `TextVectorization` con los parametros que se adaptan mejor al problema.\n",
    "\n",
    "En particular, se configurará los parametros `max_tokens` y `output_sequence_length`. \n",
    "\n",
    "- `max_tokens` - se establece en 10000, ya que el diccionario tendrá tantos tokens como palabras únicas en el texto de entrenamiento.\n",
    "- `output_sequence_length` - se utilizará el numero promedio de tokens por tweet en el dataset de entrenamiento. El numero promedio de tokens por tweet es 15.5, por lo que se establecerá `output_sequence_length` en 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find average number of tokens (words) in training Tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el vectorizador de texto a los datos\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 74,   9,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear una sentencia de ejemplo y tokenizarla\n",
    "sample_sentence = \"there is a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:\n",
      " I'm feeling so attacked https://t.co/CvkQiGr1AZ        \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 32, 707,  28, 323,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escoger una muestra aleatoria del dataset de entrenamiento y tokenizarla\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"original text:\\n {random_sentence}\\\n",
    "        \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# obtener las palabras unicas en el vocabulario\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">:toolbox: **Codificación embebida**\n",
    "\n",
    "Hasta el momento se ha convertido el texto en números enteros. Sin embargo, los números enteros no son una representación muy útil para un modelo de deep learning. Por lo tanto, se utilizará la codificación embebida para convertir los números enteros en vectores densos, que pueden ser aprendidos por el modelo.\n",
    "\n",
    "La ventaja de la codificación embebida es que puede ser aprendida durante el entrenamiento. Por lo tanto, la codificación embebida puede capturar información específica del conjunto de datos de entrenamiento. Por ejemplo, si el conjunto de datos de entrenamiento contiene palabras como \"amor\", \"feliz\" y \"triste\", la codificación embebida puede aprender que \"amor\" y \"feliz\" están relacionados y \"triste\" no está relacionado con \"amor\" o \"feliz\".\n",
    "\n",
    "Para codificar el texto, se utilizará la clase [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) de TensorFlow. Esta clase convierte una secuencia de enteros en una secuencia de vectores densos, donde cada entero representa un token en un diccionario.\n",
    "\n",
    "Los parametros de la clase `Embedding` son:\n",
    "\n",
    "- `input_dim` - el tamaño del diccionario. Por ejemplo, si el diccionario tiene 10,000 tokens, `input_dim` es 10,000. (El tamaño del diccionario se establece en el parametro `max_tokens` de la capa `TextVectorization`).\n",
    "- `output_dim` - el tamaño del vector de salida. Por ejemplo, si se establece en 128, cada token se convertirá en un vector de 128 dimensiones.\n",
    "- `embeddings_initializer` - la inicialización de los vectores de codificación embebida. Por defecto, los vectores de codificación embebida se inicializan con números aleatorios pequeños. Sin embargo, se puede utilizar una codificación embebida preentrenada, como [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
    "- `input_length` - el tamaño de la secuencia de entrada. Por ejemplo, si `input_length` es 15, la entrada debe ser una secuencia de 15 enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x25cbb859a30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\") \n",
    "\n",
    "embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":key: **Nota**: Se observa que la embedding es una capa de TensorFlow, por lo que se puede utilizar como una capa de entrada en un modelo de Keras.\n",
    "\n",
    "Ejemplo de algunas sentencias de texto y su codificación embebida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J2aQs5loxu #prebreak #best      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.00988939, -0.02746936,  0.00140302, ...,  0.03693564,\n",
       "         -0.02811122, -0.04898233],\n",
       "        [ 0.00195915, -0.03295889, -0.02535405, ..., -0.04278285,\n",
       "         -0.00316085,  0.04614298],\n",
       "        [ 0.04808098, -0.01105697, -0.01183013, ...,  0.03402085,\n",
       "         -0.00430602,  0.04447634],\n",
       "        ...,\n",
       "        [-0.04318868,  0.03441192, -0.0104339 , ...,  0.00982716,\n",
       "          0.03474809,  0.02626885],\n",
       "        [ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n",
       "          0.03332629,  0.02803668],\n",
       "        [ 0.02895654,  0.01618583,  0.03750667, ...,  0.0460008 ,\n",
       "         -0.01629362,  0.00331082]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.00988939, -0.02746936,  0.00140302,  0.00284123, -0.01472485,\n",
       "        -0.01839679,  0.0118969 ,  0.04235381, -0.00026729, -0.00642087,\n",
       "         0.02772132,  0.02748709, -0.01417947, -0.03560337,  0.00357419,\n",
       "        -0.01770699,  0.02152342,  0.03922397, -0.01609625,  0.02858197,\n",
       "        -0.00755884,  0.01439104,  0.03739473, -0.01915296, -0.01072513,\n",
       "         0.01270156, -0.01597232, -0.03835863, -0.03827304, -0.00607399,\n",
       "         0.0488129 ,  0.02241942,  0.01502916,  0.03161443,  0.04319861,\n",
       "        -0.04185198, -0.00232239, -0.02205428,  0.0469125 , -0.03193682,\n",
       "        -0.03613007,  0.01355774, -0.00302942, -0.00800582, -0.00873001,\n",
       "        -0.00984627,  0.0298288 ,  0.00029044,  0.00571251,  0.00692202,\n",
       "        -0.02142215,  0.03313327, -0.02721083,  0.03188301, -0.01273811,\n",
       "        -0.03887856, -0.03365548,  0.01385478, -0.03734227, -0.00772613,\n",
       "         0.04217112,  0.00022364,  0.00648459, -0.0071331 ,  0.04496172,\n",
       "         0.00959707,  0.04694784,  0.0113821 ,  0.02959793,  0.03298448,\n",
       "        -0.02782015, -0.01527927,  0.00467656, -0.04270205,  0.03968146,\n",
       "         0.00251913, -0.02293997, -0.02907418, -0.0001973 ,  0.0123238 ,\n",
       "         0.04800495,  0.03981445, -0.02231169, -0.03333064,  0.04854326,\n",
       "        -0.04478462, -0.00473104, -0.04193581,  0.01201888,  0.04333584,\n",
       "         0.01708932,  0.00014586, -0.00107991, -0.00276276, -0.02434686,\n",
       "         0.03231192, -0.01590673,  0.03659705,  0.0356575 ,  0.01856903,\n",
       "        -0.0380013 , -0.00366618,  0.02796532,  0.02588924,  0.04324831,\n",
       "        -0.01833223, -0.01585167,  0.02906534, -0.03386377, -0.01954178,\n",
       "        -0.0131406 , -0.03331577,  0.02442769, -0.03170791, -0.017514  ,\n",
       "        -0.03410236,  0.03045133, -0.0403619 ,  0.01897163,  0.02860156,\n",
       "        -0.00723895,  0.01443825, -0.00862253, -0.01355454,  0.03841894,\n",
       "         0.03693564, -0.02811122, -0.04898233], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " '#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J2aQs5loxu #prebreak #best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificar un unico token en codificación embebida\n",
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Modelo de deep learning para clasificación de texto\n",
    "\n",
    "- TF-IDF: Term Frequency - Inverse Document Frequency (Frecuencia de término - Frecuencia inversa del documento). Es una medida estadística que se utiliza para evaluar la importancia de una palabra en un documento en un conjunto de documentos.\n",
    "\n",
    "<a href=\"https://ibb.co/yYkdqg3\"><img src=\"https://i.ibb.co/7RpjKr9/nlp-experiments.png\" alt=\"nlp-experiments\" border=\"0\"></a>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Naive Bayes (baseline)\n",
    "\n",
    "Como en todos los experiments en machine learning, es importante crear un modelo de baseline para comparar con los modelos futuros.\n",
    "\n",
    "Para crear un modelo de baseline, se utilizará el algoritmo de Naive Bayes, que es un algoritmo de clasificación de texto muy popular. Se creará un pipeline utilizando scikit-learn utilizando el metodo TF-IDF (Term Frequency - Inverse Document Frequency) para convertir el texto en números y despues de eso modelarlos utilizando el algoritmo Multinomial Naive Bayes. La selección se realizó siguiendo el mapa de algoritmos de machine learning de scikit-learn.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" alt=\"ml-map\" border=\"0\" width=700>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8173103b97bf613cde717d81021a4a8a77760fbef76bd6423549d26b1e1d3fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
