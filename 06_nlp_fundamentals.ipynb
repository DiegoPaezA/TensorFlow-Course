{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://tinyurl.com/2khrsfbc\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with TensorFlow\n",
    "\n",
    "El objetivo principal del [procesamiento de lenguaje natural](https://tinyurl.com/y68tj6v6) (NLP) es analizar, comprender y derivar información valiosa de texto. Las aplicaciones de NLP pueden clasificarse en dos categorías principales:\n",
    "\n",
    "- Texto (Como el que se encuentra en un email, un tweet, un documento, etc.)\n",
    "- Audio (Como el que se encuentra en una grabación de voz, una canción, etc.)\n",
    "\n",
    "Por ejemplo, \n",
    "\n",
    "- Si tuviéramos un conjunto de datos de texto que contuviera reseñas de películas, podríamos usar NLP para identificar el sentimiento detrás de cada reseña (por ejemplo, positivo o negativo). \n",
    "- También podríamos usar NLP para identificar el tema de cada reseña (por ejemplo, romance, acción, etc.). Si quisieramos construir un identificador de spam, podríamos usar NLP para identificar palabras clave en un correo electrónico que podrían indicar que es spam.\n",
    "- Generación de texto: Podríamos usar NLP para generar texto, como un resumen de un artículo, un título de un artículo, una respuesta a un correo electrónico, etc.\n",
    "\n",
    "Cuando se trabaja con texto o audio en NLP los datos tienen un orden secuencial por lo tanto el tratamiento que reciben los datos es diferente al de los datos de imágenes o tabulares.\n",
    "\n",
    "**Aplicaciones del Modelado de Datos Secuenciales**\n",
    "\n",
    "Al hablar de redes neuronales densas se tenía una información de entrada y se esperaba una salida sin importar el tiempo ni el orden en el que llega la información a la red. En el modelado de datos secuenciales, el orden de la información es importante:\n",
    "\n",
    "1. En el primer ejemplo se tienen varias entradas y se obtiene una salida. \n",
    "2. En el segundo ejemplo se tiene una entrada y se quiere obtener una secuencia que represente información asociada a la acción entrante. \n",
    "3. En el ultimo ejemplo se tiene una secuencia de entrada y se espera obtener una secuencia de salida.\n",
    "\n",
    "<a href=\"https://ibb.co/m0LKSmf\"><img src=\"https://i.ibb.co/jzcYHdn/nlp-problems.png\" alt=\"nlp-problems\" border=\"0\"></a>\n",
    "\n",
    "Uno de los factores a tener en cuenta al trabajar con datos secuenciales, es que las redes neuronales densas no son capaces de capturar la información de secuencia. Por lo tanto, se requiere una red neuronal que sea capaz de capturar la información de secuencia. Para ello se utilizan las **redes neuronales recurrentes**.\n",
    "\n",
    "Las **redes neuronales recurrentes (RNN**): son redes neuronales que tienen una memoria interna que les permite capturar la información de secuencia. La arquitecura base de las RNN es la siguiente:\n",
    "\n",
    "<a href=\"https://ibb.co/dg8ycMy\"><img src=\"https://i.ibb.co/9tQjvgj/arq-rnn.png\" alt=\"arq-rnn\" border=\"0\"></a>\n",
    "\n",
    "<a href=\"https://ibb.co/TrkZFZS\"><img src=\"https://i.ibb.co/yVXDbD1/imagen-2023-02-13-105448726.png\" alt=\"imagen-2023-02-13-105448726\" border=\"0\"></a>\n",
    "\n",
    ">**En este notebook, se trabará sobre lo siguiente**:\n",
    "\n",
    "- Descargar y explorar un conjunto de datos de texto\n",
    "- Visualizar datos de texto\n",
    "- Convertir texto en números usando tokenización\n",
    "- Convertir texto tokenizado en vectores usando codificación embebida\n",
    "- Modelar un dataset de texto\n",
    "- Predecir el sentimiento de una oración\n",
    "    - Iniciar con una referencia (TF-IDF)\n",
    "    - Construir diferentes modelos de texto de deep learning\n",
    "        - Dense, LSTM, GRU, Conv1D, Transfer Learning\n",
    "- Comparar el desempeño de cada modelo\n",
    "- Combinar los en un ensamble\n",
    "- Guardar y cargar un modelo entrenado\n",
    "- Encontrar las peores predicciones.\n",
    "\n",
    "<a href=\"https://ibb.co/yYkdqg3\"><img src=\"https://i.ibb.co/7RpjKr9/nlp-experiments.png\" alt=\"nlp-experiments\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2060 with Max-Q Design (UUID: GPU-9970422a-f4b7-7ab0-4f13-419e26e877d3)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descargar y explorar un dataset de texto\n",
    "\n",
    "Se utilizará el dataset [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) que contiene Tweets acerca de desastres naturales. El objetivo es predecir si un tweet es acerca de un desastre real o no.\n",
    "\n",
    "- Un Tweet **Real** es acerca de un desastre real, por ejemplo:\n",
    "> Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n",
    "\n",
    "- Un Tweet **No Real** no es acerca de un desastre real pueden ser sobre cualquier cosa, por ejemplo:\n",
    "> 'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n",
    "\n",
    "Al descomprimir el dataset se obtendran tres archivos:\n",
    "\n",
    "- `sample_submission.csv` - un archivo de muestra de la estructura de envío de predicciones en las competencias de Kaggle.\n",
    "- `train.csv` - el conjunto de datos de entrenamiento que contiene los tweets reales y no reales de desastres etiquetados.\n",
    "- `test.csv` - el conjunto de datos de prueba que contiene los tweets reales y no reales que se deben predecir.\n",
    "\n",
    "\n",
    "### 1.1 Visualizar un dataset de texto\n",
    "\n",
    "Una vez que se cuenta con un dataset para trabajar, es una buena idea visualizarlo para ver si hay algo que se pueda entender de él. El dataset se encuentra en archivos `.csv`, una forma sencilla de hacerlos legibles es usando la función `pd.read_csv()` de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"datasets/nlp_real_not_dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"datasets/nlp_real_not_dataset/test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "los datos descargados seguramente se encuentran mezclados. Pero para asegurarnos es una buena idea volverlos a mezclar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de entrenamiento contienen la columna `target` que indica si un tweet es real o no real. Esta columna será la etiqueta que se usará para entrenar el modelo. El dataset de prueba no contiene esta columna, ya que es lo que se debe predecir.\n",
    "\n",
    "La estructura que se quiere implementar es la siguiente:\n",
    "\n",
    "> Inputs (text column) -> Machine Learning Algorithm -> Outputs (target column)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-text-classification-inputs-and-outputs.png\" alt=\"nlp\" border=\"0\" width = 800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test data doesn't have a target (that's what we'd try to predict)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que se cuenta con dos valores objetivo (0 y 1), se trata de un problema de clasificación binaria. El dataset se encuentra desbalanceado, con un 60% de tweets de clase 0 y un 40% de tweets de clase 1.\n",
    "\n",
    "donde: \n",
    "\n",
    "- 1 = Un tweet es acerca de un desastre real\n",
    "- 0 = Un tweet no es acerca de un desastre real\n",
    "\n",
    "El dataset de entrenamiento contiene 7613 tweets, el dataset de prueba contiene 3263 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "# How many samples total?\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "'Among other main factors behind pedestrian fatalities are people failing to yield to a car' http://t.co/dgUL7FfJt2\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "What's up man?\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "America like South Africa is a traumatised sick country - in different ways of course - but still messed up.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Cape Coral city leaders take part in mock hurricane training http://t.co/gtYCQyFuam http://t.co/qwd5PvGjbO\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "'If I'd have had a long coat to hand I'd have worn it. The certainty of armageddon bears a sense of occasion.'\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualización de algunos de los ejemplos de entrenamiento\n",
    "import random\n",
    "random_index = random.randint(0,len(train_df)-5)\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _,text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\"\n",
    "          if target>0 else \"(not real disaster)\"  \n",
    "    )\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Separar el dataset en conjuntos de entrenamiento y validación\n",
    "\n",
    "Debido a que el dataset de prueba no contiene la columna `target`, se debe separar el dataset de entrenamiento en dos conjuntos: uno para entrenar el modelo y otro para validar el modelo. Para ello se utilizará la función [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42) # random state for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificar los primeros 10 ejemplos\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convertir texto en números\n",
    "\n",
    "Para poder alimentar texto a un modelo de deep learning, es necesario convertirlo en números. Hay varias formas de hacer esto, las más comunes son:\n",
    "\n",
    "- **Tokenización**: Mapeado directo de palabras, caracteres o símbolos a números enteros. Hay tres niveles de tokenización:\n",
    "    - **Palabra**: convertir cada palabra en una secuencia en un número entero. Con la sentencia \"i love tensorflow\", la palabra \"i\" se convertiría en el número 1, \"love\" en el número 2 y \"tensorflow\" en el número 3.\n",
    "    - **Caracter**: convertir cada caracter en una secuencia en un número entero. Como convertir el abecedarios A-Z en números del 1 al 26.\n",
    "    - **Subpalabra**: convertir cada subpalabra en una secuencia en un número entero. Involucra dividir las palabras en subpalabras, por ejemplo, \"tensorflow\" se dividiría en \"ten\", \"sor\", \"flo\", \"w\".\n",
    "\n",
    "\n",
    "- **Codificación embebida**: Es una representación de lenguaje natural que puede ser aprendida. La representación se da en forma de un vector de características de longitud fija, donde cada característica representa un aspecto del lenguaje. Por ejemplo, la palabra \"dance\" puede ser presentada por un vector de 5 dimensiones [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. Es importante resaltar que el tamaño del vector es ajustable. Existen dos formas de usar la codificación embebida:\n",
    "    - **Codificación embebida preentrenada**: Se utiliza una codificación embebida que ya ha sido entrenada en un conjunto de datos grande. Por ejemplo, la codificación embebida preentrenada de [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
    "    - **Codificación embebida entrenada**: Se entrena una codificación embebida desde cero en un conjunto de datos específico. Por ejemplo, se entrena una codificación embebida en un conjunto de datos de tweets.\n",
    "\n",
    "<a href=\"https://ibb.co/0sJqwkD\"><img src=\"https://i.ibb.co/Z2TdZ3L/tokens.png\" alt=\"tokens\" border=\"0\"></a>\n",
    "\n",
    ">:key: **Nota**: Qué nivel de tokenización o codificación embebida se debe utilizar? \n",
    "\n",
    "Depende del problema que se está tratando de resolver. Es importante experimentar con diferentes niveles de tokenización y codificación embebida para ver cuál funciona mejor para tu problema.\n",
    "\n",
    "Algunas herramientas de codificación embebida preentrenadas:\n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "- [Word2Vec](https://jalammar.github.io/illustrated-word2vec/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">:toolbox: **Vectorización de texto (tokenización)**\n",
    "\n",
    "Para vectorizar el texto, se utilizará la clase [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) de TensorFlow. Esta clase convierte una secuencia de caracteres en una secuencia de enteros, donde cada entero representa un token en un diccionario.\n",
    "\n",
    "La capa `TextVectorization` toma los siguientes parametros: \n",
    "\n",
    "- `max_tokens` - el número máximo de tokens a tener en el diccionario. Si se establece en `None`, el diccionario tendrá tantos tokens como palabras únicas en el texto de entrenamiento.\n",
    "- `standardize` - la función de normalización a aplicar a los datos de entrada. Por defecto, la función de normalización convierte el texto a minúsculas y elimina la puntuación.\n",
    "- `split` - la función de separación a aplicar a los datos de entrada. Por defecto, \"whitespace\" separa el texto en palabras.\n",
    "- `ngrams` - Cuantas palabras se deben combinar en un solo token. Por ejemplo, si `ngrams` es 2, la frase \"I love TensorFlow\" se convertiría en \"I love\", \"love TensorFlow\".\n",
    "- `output_mode` - el modo de salida de la capa. Por defecto, \"int\" devuelve una secuencia de enteros (cada entero representa un token en el diccionario). `int` (mapeado entero), `binary` (one-hot encoding), `count` o `tf-idf`.\n",
    "- `output_sequence_length` - Tamaño de la secuencia de salida tokenizada. Por ejemplo, si `output_sequence_length` es 10, la secuencia de salida tendrá 10 tokens, si la secuencia de entrada tiene menos de 10 tokens, se rellenará con ceros. Si la secuencia de entrada tiene más de 10 tokens, se truncará a los primeros 10 tokens.\n",
    "- `pad_to_max_tokens` - Si es `True`, la secuencia de salida se rellenará con ceros hasta que tenga el tamaño de `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La inicialización anterior de la capa `TextVectorization` utiliza los parametros por defecto. A continuación se creará una nueva capa `TextVectorization` con los parametros que se adaptan mejor al problema.\n",
    "\n",
    "En particular, se configurará los parametros `max_tokens` y `output_sequence_length`. \n",
    "\n",
    "- `max_tokens` - se establece en 10000, ya que el diccionario tendrá tantos tokens como palabras únicas en el texto de entrenamiento.\n",
    "- `output_sequence_length` - se utilizará el numero promedio de tokens por tweet en el dataset de entrenamiento. El numero promedio de tokens por tweet es 15.5, por lo que se establecerá `output_sequence_length` en 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find average number of tokens (words) in training Tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el vectorizador de texto a los datos\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 74,   9,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear una sentencia de ejemplo y tokenizarla\n",
    "sample_sentence = \"there is a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:\n",
      " #Nuclear policy of #Japan without responsibility about Nuclear #Disaster will repeat same #failure.\n",
      "#annonymous #guardian #NYTimes #Reuters        \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 105,  595,    6,  224,  228, 2745,   54,  105,   75,   38, 3477,\n",
       "         726,  320,    1, 1845]], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Escoger una muestra aleatoria del dataset de entrenamiento y tokenizarla\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"original text:\\n {random_sentence}\\\n",
    "        \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# obtener las palabras unicas en el vocabulario\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">:toolbox: **Codificación embebida**\n",
    "\n",
    "Hasta el momento se ha convertido el texto en números enteros. Sin embargo, los números enteros no son una representación muy útil para un modelo de deep learning. Por lo tanto, se utilizará la codificación embebida para convertir los números enteros en vectores densos, que pueden ser aprendidos por el modelo.\n",
    "\n",
    "La ventaja de la codificación embebida es que puede ser aprendida durante el entrenamiento. Por lo tanto, la codificación embebida puede capturar información específica del conjunto de datos de entrenamiento. Por ejemplo, si el conjunto de datos de entrenamiento contiene palabras como \"amor\", \"feliz\" y \"triste\", la codificación embebida puede aprender que \"amor\" y \"feliz\" están relacionados y \"triste\" no está relacionado con \"amor\" o \"feliz\".\n",
    "\n",
    "Para codificar el texto, se utilizará la clase [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) de TensorFlow. Esta clase convierte una secuencia de enteros en una secuencia de vectores densos, donde cada entero representa un token en un diccionario.\n",
    "\n",
    "Los parametros de la clase `Embedding` son:\n",
    "\n",
    "- `input_dim` - el tamaño del diccionario. Por ejemplo, si el diccionario tiene 10,000 tokens, `input_dim` es 10,000. (El tamaño del diccionario se establece en el parametro `max_tokens` de la capa `TextVectorization`).\n",
    "- `output_dim` - el tamaño del vector de salida. Por ejemplo, si se establece en 128, cada token se convertirá en un vector de 128 dimensiones.\n",
    "- `embeddings_initializer` - la inicialización de los vectores de codificación embebida. Por defecto, los vectores de codificación embebida se inicializan con números aleatorios pequeños. Sin embargo, se puede utilizar una codificación embebida preentrenada, como [GloVe](https://nlp.stanford.edu/projects/glove/).\n",
    "- `input_length` - el tamaño de la secuencia de entrada. Por ejemplo, si `input_length` es 15, la entrada debe ser una secuencia de 15 enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x238b9890280>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\") \n",
    "\n",
    "embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":key: **Nota**: Se observa que la embedding es una capa de TensorFlow, por lo que se puede utilizar como una capa de entrada en un modelo de Keras.\n",
    "\n",
    "Ejemplo de algunas sentencias de texto y su codificación embebida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "DTN Brazil: Experts in France begin examining airplane debris found on Reunion Island: French air accident exp... http://t.co/M9IG3WQ8Lq      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-1.3418257e-02, -1.5239418e-02, -2.2546673e-02, ...,\n",
       "          1.4180765e-03, -3.9529037e-02, -3.4851659e-02],\n",
       "        [-3.6835194e-02, -3.2670140e-02, -1.0911129e-02, ...,\n",
       "          4.2290155e-02, -3.6159564e-02,  1.8233061e-04],\n",
       "        [ 3.2823417e-02,  2.2264209e-02, -2.9418886e-02, ...,\n",
       "         -3.0449403e-02, -1.2066126e-02,  1.0320615e-02],\n",
       "        ...,\n",
       "        [ 8.6771250e-03, -1.9490719e-05, -4.0783606e-02, ...,\n",
       "          1.5042376e-02,  1.5071120e-02,  8.8122115e-03],\n",
       "        [-1.6325913e-02, -3.6601674e-02,  2.9070247e-02, ...,\n",
       "          1.5249323e-02,  3.4499016e-02,  4.0705215e-02],\n",
       "        [-2.2900343e-02,  4.0046167e-02,  7.0909038e-03, ...,\n",
       "          3.6726784e-02, -4.0531863e-02, -2.5834311e-02]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.01341826, -0.01523942, -0.02254667,  0.0494989 ,  0.00216525,\n",
       "        -0.0399882 ,  0.00080553,  0.01476188,  0.03615234, -0.00884441,\n",
       "         0.00621294, -0.02955726,  0.00980123,  0.01277976, -0.03157198,\n",
       "         0.00294412,  0.02901384, -0.04287526, -0.04256399,  0.03591888,\n",
       "        -0.04160985,  0.03295306, -0.03178471, -0.04270699, -0.01254467,\n",
       "        -0.03199406,  0.00422915, -0.01716496,  0.03225711,  0.02024985,\n",
       "         0.00232749,  0.04223504,  0.00964341, -0.01311073, -0.00329622,\n",
       "        -0.01189204, -0.01893516,  0.00376328, -0.01839433,  0.03581586,\n",
       "         0.04048808,  0.02670585,  0.02073118,  0.00023675, -0.02499862,\n",
       "        -0.04086368, -0.03574042, -0.02921354,  0.00548168,  0.02125393,\n",
       "         0.03762517, -0.04518021, -0.0409437 ,  0.029798  ,  0.01770559,\n",
       "         0.00280501,  0.00791682, -0.03655709,  0.02362288,  0.03642707,\n",
       "        -0.0153024 ,  0.00849842,  0.04856699,  0.0005394 ,  0.03691434,\n",
       "        -0.03200384, -0.00154785,  0.03311023, -0.02303641,  0.00659452,\n",
       "         0.01671362,  0.0401844 , -0.02989746,  0.01172427,  0.0043266 ,\n",
       "        -0.01014818,  0.0055742 , -0.02715824,  0.0102226 , -0.04452337,\n",
       "         0.01142269, -0.02887641,  0.00553133, -0.02590504,  0.0350236 ,\n",
       "        -0.00500716, -0.00595337, -0.01995603,  0.02940241, -0.01821731,\n",
       "        -0.02583588, -0.01408648,  0.0201504 ,  0.0335434 , -0.04472008,\n",
       "        -0.02761248,  0.04755821, -0.00116609, -0.02528777,  0.02496601,\n",
       "        -0.00733799, -0.04409865, -0.02077821, -0.00690372, -0.01877717,\n",
       "         0.04266268, -0.03254855,  0.01601443,  0.01614847, -0.00561534,\n",
       "         0.00745501,  0.00523942, -0.03176825, -0.00150438, -0.04294132,\n",
       "         0.00730207, -0.04777099,  0.04351056,  0.00117433,  0.03278336,\n",
       "        -0.03820665,  0.04456751, -0.00368005, -0.0194006 , -0.00126324,\n",
       "         0.00141808, -0.03952904, -0.03485166], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " 'DTN Brazil: Experts in France begin examining airplane debris found on Reunion Island: French air accident exp... http://t.co/M9IG3WQ8Lq')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verificar un unico token en codificación embebida\n",
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Modelo de deep learning para clasificación de texto\n",
    "\n",
    "- TF-IDF: Term Frequency - Inverse Document Frequency (Frecuencia de término - Frecuencia inversa del documento). Es una medida estadística que se utiliza para evaluar la importancia de una palabra en un documento en un conjunto de documentos.\n",
    "\n",
    "<a href=\"https://ibb.co/yYkdqg3\"><img src=\"https://i.ibb.co/7RpjKr9/nlp-experiments.png\" alt=\"nlp-experiments\" border=\"0\"></a>\n",
    "\n",
    "Para crear los modelos se seguiran los siguientes pasos:\n",
    "\n",
    "1. Crear un modelo de deep learning.\n",
    "2. Compilar el modelo.\n",
    "3. Ajustar el modelo.\n",
    "4. Evaluar el modelo.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Naive Bayes (baseline)\n",
    "\n",
    "Como en todos los experiments en machine learning, es importante crear un modelo de baseline para comparar con los modelos futuros.\n",
    "\n",
    "Para crear un modelo de baseline, se utilizará el algoritmo de Naive Bayes, que es un algoritmo de clasificación de texto muy popular. Se creará un pipeline utilizando scikit-learn utilizando el metodo TF-IDF (Term Frequency - Inverse Document Frequency) para convertir el texto en números y despues de eso modelarlos utilizando el algoritmo Multinomial Naive Bayes. La selección se realizó siguiendo el mapa de algoritmos de machine learning de scikit-learn.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" alt=\"ml-map\" border=\"0\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Crear tokenizacion y el pipeline del modelo\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convertir las palabras en numeros utilizando tfidf\n",
    "    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit el pipeline a los datos\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo base\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Baseline Accuracy of: {baseline_score*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper_functions as hf\n",
    "baseline_results = hf.calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Feed-forward neural network (dense model)\n",
    "\n",
    "El primer modelo de deep learning que se creará será un modelo de red neuronal densa (feed-forward neural network) compuesto por una sola capa.\n",
    "\n",
    "El modelo tomará el texto y las etiquetas como entrada, despues se tokenizara el texto, se crear un embedding, posteriormente se calculara el promedio de los embeddings (usando Global Average Pooling) y finalmente se pasara el promedio a una capa totalmente conectada con una unidad de salida y una función de activación sigmoide.\n",
    "\n",
    "Se registran los resultados de los diferentes modelos utilizando la función `create_tensorboard_callback()` del archivo `helper_functions.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Crear directorio para salvar los logs de tensorboard\n",
    "\n",
    "SAVE_DIR = \"training_logs/RNN_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un modelo con la api funcional\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\") # entradas son string de 1-dimensión\n",
    "x = text_vectorizer(inputs) # convertir el texto en números\n",
    "x = embedding(x) # crear un embedding del texto tokenizado\n",
    "x = layers.GlobalAveragePooling1D()(x) # condensa la salida de cada token a un vector de 128, si no se utiliza, se obtiene una predicción por cada token\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: training_logs/RNN_text/simple_dense_model_1/20230225-183506\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 3s 8ms/step - loss: 0.6094 - accuracy: 0.6916 - val_loss: 0.5357 - val_accuracy: 0.7572\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.4410 - accuracy: 0.8189 - val_loss: 0.4691 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.3463 - accuracy: 0.8605 - val_loss: 0.4590 - val_accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.2848 - accuracy: 0.8923 - val_loss: 0.4641 - val_accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 7ms/step - loss: 0.2380 - accuracy: 0.9118 - val_loss: 0.4767 - val_accuracy: 0.7874\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"simple_dense_model_1\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4767 - accuracy: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47668465971946716, 0.787401556968689]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40488204],\n",
       "       [0.74433124],\n",
       "       [0.997895  ],\n",
       "       [0.10889999],\n",
       "       [0.11143529],\n",
       "       [0.93556094],\n",
       "       [0.9134595 ],\n",
       "       [0.9925345 ],\n",
       "       [0.97156817],\n",
       "       [0.26570338]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observar las 10 primeras predicciones\n",
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convertir las predicciones del modelo en formato de labels\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.74015748031496,\n",
       " 'precision': 0.7914920592553047,\n",
       " 'recall': 0.7874015748031497,\n",
       " 'f1': 0.7846966492209201}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcular los resultados del modelo 1\n",
    "model_1_results = hf.calculate_results(y_true=val_labels, \n",
    "                                       y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">:eye: **Nota:** Se observa que el modelo de Naive Bayes tiene un accuracy de 0.79, mientras que el modelo de deep learning tiene un accuracy de 0.78. Es decir que el modelo **baseline** es mejor que el modelo de deep learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los embeddings aprendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 1 summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00073165,  0.01504799, -0.03425453, ..., -0.0440354 ,\n",
       "         -0.01042281,  0.01876434],\n",
       "        [ 0.04135861, -0.03945085, -0.03811941, ...,  0.00464737,\n",
       "          0.03163553,  0.02928301],\n",
       "        [ 0.00684031,  0.05363133, -0.00241554, ..., -0.07082178,\n",
       "         -0.04750703,  0.01448254],\n",
       "        ...,\n",
       "        [-0.03301444, -0.0052493 , -0.04209725, ...,  0.02028764,\n",
       "          0.00308807,  0.02215792],\n",
       "        [ 0.00692343,  0.05942352, -0.01975194, ..., -0.06199061,\n",
       "         -0.01018393,  0.03510419],\n",
       "        [-0.0372346 ,  0.06267187, -0.07451147, ..., -0.02367218,\n",
       "         -0.0864333 ,  0.01742156]], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener la matrix de pesos de la capa embedding\n",
    "# estos son los pesos que representan a cada token en los datos de entrenamiento, \n",
    "# que han sido aprendidos durante 5 epocas\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()\n",
    "embed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Shape tiene que tener el mismo tamaño del vocabulario y tamaño de sailda\n",
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos la matrix de embeddings aprendidos por el modelo de deep learning. Para visualizar los embeddings, se utilizará la herramienta [Embedding Projector](https://projector.tensorflow.org/).\n",
    "\n",
    ":key: TensorFlow tiene una guía sobre word embeddings que se puede encontrar [aquí](https://www.tensorflow.org/tutorials/text/word_embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embedding Files\n",
    "# Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
    "import io\n",
    "\n",
    "# # Create output writers\n",
    "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# # Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "   if num == 0: \n",
    "      continue # skip padding token\n",
    "   vec = embed_weights[num]\n",
    "   out_m.write(word + \"\\n\") # write words to file\n",
    "   out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "# Download files locally to upload to Embedding Projector\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "   pass\n",
    "else:\n",
    "   files.download(\"embeddings_projector/embedding_vectors.tsv\")\n",
    "   files.download(\"embeddings_projector/embedding_metadata.tsv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se han descargado los archivos, se procede a cargarlos en el [Embedding Projector](https://projector.tensorflow.org/).\n",
    "\n",
    "1. Cargar el archivo `vectors.tsv` en el campo \"Load data\".\n",
    "2. Cargar el archivo `metadata.tsv` en el campo \"Load metadata\".\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales Recurrentes (RNN)\n",
    "\n",
    "La premisa de una RNN es sencilla: usa información de la secuencia anterior para ayudar a predecir la siguiente entrada. Por ejemplo, si se está tratando de predecir la próxima palabra en una oración, la RNN puede utilizar información sobre la palabra anterior para ayudar a predecir la siguiente palabra. De esta manera, la RNN puede aprender el contexto de una secuencia de entrada. \n",
    "\n",
    "Este concepto es especialmente útil cuando se está trabajando con secuencias como texto, audio o video, donde el orden de las entradas es importante.\n",
    "\n",
    "Cuando una RNN observa una secuencia de texto, los patrones que aprende se actualizan basados en la secuencia de entrada. \n",
    "\n",
    "Por ejemplo, observando las dos sentencias de texto a continuación:\n",
    "\n",
    "- Massive earthquake last week, no?\n",
    "- No massive earthquake last week.\n",
    "\n",
    "Ambas frances contienen las mismas palabras pero tienen diferente significado. El orden de las palabras determina el significado de la frase.\n",
    "\n",
    "Las RNNs se pueden utilizar para un número de problemas basados en secuencias:\n",
    "\n",
    "- **One to one**: una entrada, una sailda, como clasificación de imágenes.\n",
    "- **One to many**: una entrada, varias salidas, como subtítulado de imagenes (imagen -> texto).\n",
    "- **Many to one**: varias entradas, una salida, como clasificación de texto (texto -> etiqueta).\n",
    "- **Many to many**: varias entradas, varias salidas, como traducción de texto (texto -> texto) o audio a texto (audio -> texto).\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" alt=\"rnn\" border=\"0\" width=700>\n",
    "\n",
    "Al trabajar con RNNs, es posible encontrar las siguientes variantes:\n",
    "\n",
    "- LSTM (Long Short-Term Memory)\n",
    "- GRU (Gated Recurrent Unit)\n",
    "- Bidirectional RNNs.\n",
    "\n",
    ":book: **Referencias**:\n",
    "\n",
    "- [MIT Deep Learning Lecture on Recurrent Neural Networks ](https://youtu.be/SEnXr6v2ifU)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LSTM\n",
    "\n",
    "LSTM( Long Short-Term Memory) es una variante de RNN que intenta resolver el problema del gradiente desvaneciente. Las LSTM son capaces de aprender a largo plazo y son muy útiles para tareas de procesamiento de lenguaje natural.\n",
    "\n",
    "Las LSTM al igual que la RNN básica tienen una estructura de cadena, pero el modulo de memoria tiene una estructura diferente. En lugar de tener una sola capa de red neuronal, hay cuatro, que interactúan de una manera muy especial:\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" alt=\"lstm\" border=\"0\" width=600>\n",
    "\n",
    "**Notación**: En el diagrama anterior, cada línea lleva un vector entero, desde la salida de un nodo a la entrada de otro. Los circulos rosados representan una operación de pointwise (elemento a elemento), como suma de vectores, mientras que las cajas amarillas son capas entrenadas de una red neuronal. Las lineas que se fusionan denotan concatenación, mientras que una línea que se bifurca denota que su contenido se copia y las copias van a lugares distintos.\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png\" alt=\"lstm2\" border=\"0\" width=600>\n",
    "\n",
    "**La idea central detrás de las LSTMs**: La clave de los LSTM es el estado de la celda, la línea horizontal que atraviesa la parte superior del diagrama.\n",
    "\n",
    "El estado de la celda es como una cinta transportadora. Se ejecuta directamente a lo largo de toda la cadena, con solo algunas interacciones lineales menores. Es muy fácil que la información fluya sin cambios.\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" alt=\"lstm3\" border=\"0\" width=600>\n",
    "\n",
    "La LSTM tiene la habilidad de remover o adicionar información al estado de la celda, controlado cuidadosamente por estructuras llamadas compuertas (gates). Las compuertas son una manera de permitir que la información fluya o se bloquee en función de lo que la LSTM desea aprender. Se encuentran compuestas de una capa de red neuronal sigmoide y una multiplicación elemento a elemento.\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png\" alt=\"lstm4\" border=\"0\" width=150>\n",
    "\n",
    "La capa sigmoide genera un número entre 0 y 1, describiendo cuánto de cada unidad debe ser permitida a través. Una 1 significa \"permitir completamente\", mientras que una 0 significa \"bloquear completamente\".\n",
    "\n",
    "Una LSTM tiene tres compuertas que controlan su estado de celda:\n",
    "\n",
    "**Paso a paso de la LSTM:**\n",
    "\n",
    "El primer paso en la LSTM es decidir qué información vamos a olvidar del estado de la celda anterior. Esto se hace con una compuerta de olvido, que es una capa sigmoide que aprende qué información debe ser olvidada basada en la entrada y el estado de la celda anterior.\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" alt=\"lstm5\" border=\"0\" width=600>\n",
    "\n",
    "El siguiente paso es decidir qué nueva información se va a almacenar en el estado de la celda. Esto se hace con dos compuertas: una compuerta de entrada y una compuerta de salida. La compuerta de entrada decide que valores se actualizarán. Despues, una capa $tanh$ crea un vector de nuevos valores candidatos $\\~{C}_{t}$, que se multiplican por la compuerta de entrada, para decidir qué valores se van a almacenar en el estado de la celda.\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" alt=\"lstm6\" border=\"0\" width=600>\n",
    "\n",
    "Finalmente, es el momento de actualizarr el estado de la celda anterior, $C_{t-1}$, a la nueva celda actualizada, $C_{t}$. Primero, multiplicamos el estado de la celda anterior $f_{t}$ por la compuerta de olvido, para decidir qué parte del estado de la celda anterior se va a olvidar. Despues, sumamos el nuevo estado de la celda candidato, $i_{t}*\\~{C}_{t}$, multiplicado por la compuerta de entrada, para decidir qué parte del nuevo estado de la celda candidato se va a almacenar en el estado de la celda actual.\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" alt=\"lstm7\" border=\"0\" width=600>\n",
    "\n",
    "Finalmente, es necesario decidir cual será la salida. La salida se basará en el estado de la celda, pero será una versión filtrada. Primero se ejecuta una capa sigmoide, para decidir que parte de la celda se va a filtrar. Despues, se ejecuta una capa $tanh$, para crear un vector de valores entre -1 y 1, que se multiplica por la salida de la capa sigmoide, para decidir que valores se van a mostrar como salida.\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" alt=\"lstm8\" border=\"0\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de la definición de las LSTM, iniciaremos la implementación. Para utilizar las celdas LSTM, se utilizará `tensorflow.keras.layers.LSTM()`.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/08-RNN-architecture-coloured-block-edition.png\" alt=\"lstm9\" border=\"0\" width=800>\n",
    "\n",
    "El modelo tendrá la siguiente estructura:\n",
    "\n",
    "> Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "\n",
    "La principal diferencia será que se agregará una capa LSTM entre el embedding y la salida. Se creará una nueva capa de embedding para asegurarse de que no se está reutilizando el embedding de la capa anterior. La capa de `text_vectorizer`se puede reutilizar, ya que no se está entrenando.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\")\n",
    "\n",
    "# Create LSTM Model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "print(x.shape)\n",
    "outputs=layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: training_logs/RNN_text/model_2_LSTM/20230225-183636\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 12ms/step - loss: 0.5100 - accuracy: 0.7416 - val_loss: 0.4566 - val_accuracy: 0.7822\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.3176 - accuracy: 0.8717 - val_loss: 0.5138 - val_accuracy: 0.7756\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.2201 - accuracy: 0.9152 - val_loss: 0.5858 - val_accuracy: 0.7677\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.1556 - accuracy: 0.9428 - val_loss: 0.6041 - val_accuracy: 0.7743\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.1076 - accuracy: 0.9594 - val_loss: 0.8746 - val_accuracy: 0.7507\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_2_history = model_2.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
    "                                                                     \"model_2_LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((762, 1),\n",
       " array([[0.007126  ],\n",
       "        [0.78736776],\n",
       "        [0.9996376 ],\n",
       "        [0.0567918 ],\n",
       "        [0.00258219],\n",
       "        [0.9996238 ],\n",
       "        [0.9217022 ],\n",
       "        [0.9997993 ],\n",
       "        [0.9994954 ],\n",
       "        [0.664574  ]], dtype=float32))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round out predictions and reduce to 1-dimensional array\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 75.06561679790026,\n",
       " 'precision': 0.7510077975908164,\n",
       " 'recall': 0.7506561679790026,\n",
       " 'f1': 0.7489268622514025}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate LSTM model results\n",
    "model_2_results = hf.calculate_results(y_true=val_labels,\n",
    "                                    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to compare our baseline results to new model results\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "  for key, value in baseline_results.items():\n",
    "    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 75.07, Difference: -4.20\n",
      "Baseline precision: 0.81, New precision: 0.75, Difference: -0.06\n",
      "Baseline recall: 0.79, New recall: 0.75, Difference: -0.04\n",
      "Baseline f1: 0.79, New f1: 0.75, Difference: -0.04\n"
     ]
    }
   ],
   "source": [
    "# Compare model 2 to baseline\n",
    "compare_baseline_to_new_results(baseline_results, model_2_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8173103b97bf613cde717d81021a4a8a77760fbef76bd6423549d26b1e1d3fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
